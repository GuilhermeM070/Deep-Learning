# -*- coding: utf-8 -*-
"""Funções de Ativação-Redes_Neurais.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Irkv9dwX8U0JOe0zIZ83qO0dGrtGAdZn
"""

import torch
from torch import nn

from sklearn.datasets import make_classification
import matplotlib.pyplot as plt
import numpy as np

np.random.seed(46)

X, Y = make_classification(n_features=2, n_redundant=0, n_informative=1,
                             n_clusters_per_class=1)

def plotmodel(w1, w2, b):

  plt.scatter(X[:, 0], X[:, 1], marker='o', c=Y,
           edgecolor='k')

  xmin, xmax = plt.gca().get_xlim()
  ymin, ymax = plt.gca().get_ylim()

  x = np.linspace(-2, 4, 50)
  y = (-w1*x -b)/w2

  plt.axvline(0, -1, 1, color='k', linewidth=1)
  plt.axhline(0, -2, 4, color='k', linewidth=1)
  plt.plot(x, y, label='_nolegend_')

  plt.xlim(xmin, xmax)
  plt.ylim(ymin, ymax)

w1 = 5 #a
w2 = 1  #b
b  = 1.2  #c
plotmodel(w1, w2, b)

p = (-1, 1)
print(w1 * p[0] + w2 * p[1] + b)

perceptron = nn.Linear(2,1)
sigmoid = nn.Sigmoid()

print(perceptron.weight.data)
print(perceptron.bias.data)

perceptron.weight = nn.Parameter(torch.Tensor([[w1, w2]]))
perceptron.bias = nn.Parameter(torch.Tensor([1.2]))

print(perceptron.weight)
print(perceptron.bias)

markers = ['^', 'v', '<', '>']
colors = ['orange', 'r', 'g', 'b']

plt.figure(figsize=(8, 5))
plotmodel(w1, w2, b)
for k, idx in enumerate([17, 21, 43, 66]):
  x = torch.Tensor(X[idx])

  ret = perceptron(x)
  act = sigmoid(ret)

  act_limiar = 0 if ret.data < 0 else 1

  label = 'Ret: {:5.2f}'.format(ret.data.numpy()[0]) + ' Limiar: {:4.2f}'.format(act_limiar) + ' Act: {:5.2f}'.format(act.data.numpy()[0])
  plt.plot(x[0], x[1], marker=markers[k], color=colors[k], markersize=10, label=label)

plt.legend()
plt.show()

# Descomente uma das linhas abaixo e rode novamente a célula anterior

#activation = nn.ReLU()
# activation = nn.Tanh()

